# BEST: Benchmarking Efficiency in Space and Time for LLM-Generated Code

## Abstract

Large language models (LLMs) have revolutionized research in software engineering, and among various tasks, LLM-based code synthesis is promising. A very recent line of benchmarks aims to evaluate LLM-generated codes in time efficiency, beyond their correctness. However, space, another vital aspect of code efficiency,is rarely evaluated in prior benchmarks. To fill in the gap, this paper introduces BEST, the first benchmark for evaluating the efficiency of LLM-generated codes in both time and space. It comprises 400 C++ coding tasks that are rigorously constructed by experts. In addition, we propose a fine-grained subtask-based evaluation scheme by dividing each task into multiple subtasks, with different input scales and difficulties. Each subtask is then accompanied by an expert-crafted standard implementation as the efficiency baseline, that achieves the Pareto optimum. Building on BEST,we introduce a unified and novel dual-indicator (time and space) metric, named dual@k, generalizing the notion of the standard pass@k metric and building on a careful and novel construction of a weight matrix of subtasks. Through extensive experiments with dual@k across 31 LLMs on BEST, our evaluation demonstrates that while LLMs exhibit weak capabilities in generating time-efficient code, their capabilities in space-efficient code generation are even worse.

## Installation and Initialization

Run the following code to complete the environment setup:

```bash
cd BEST
pip install -r requirements.txt
```

`init/model.yaml` is the configuration file for the model, and `.env` stores the api_key.

`data/data.csv` contains the problem data, and `data/test_cases` stores the corresponding test data for the problems.

## Evaluation

The evaluation can be divided into three parts: generation, execution, and data processing.

### Generation and Execution

`ask.py` calls the API to query the large model and processes the conversation content to obtain the code.

`run.py` executes the code generated by the large model and stores the results in the corresponding `result.json` file.

Run `main.py` to conveniently complete both the code querying and execution in one step:

```bash
python main.py
```

### Data Processing

`export_dual_at_k.py` processes the `result.json` files collectively to obtain the $pass@k$ matrix required for calculating the $dual@k$ value, and the results are stored in `process-data/dual_at_k`.

`dual_at_k.py` calculates the weight matrix based on different values of $\tau$ and $\sigma$, and performs an inner product with the $pass@k$ matrix to obtain the $dual@k$ values for each model.

```bash
python export_dual_at_k.py
cd process-data
python dual_at_k.py
```